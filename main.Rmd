---
title: "main"
author: "Horst-Heinen"
date: "20/05/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

# IMPROVING DATA COMPATIBILITY ON SPECTRAL LIBRARIES: SOIL ORGANIC CARBON PEDOTRANSFER FUNCTIONS FOR SOUTHERN BRAZIL

## Objetive

Experimental procedure to harmonize SOC data (DC, WCt, and WCc) from the southern Brazilian spectral library considering its geographic, topography, pedologic, and spectra data.


```{r}
library(raster)
library(sp)
library(tidyr)
library(magrittr)
library(factoextra)
library(ggplot2)
library(gridExtra)
library(grid)
library(gridGraphics)
library(latticeExtra)
library(lattice)
library(dplyr)
library(glue)
```


### Soil data
The dataset is stored in the folder `data`.

```{r}
data <- read.csv("data/data.csv", sep = ";")
```

# Pedotransfer functions

## Model formulation

We test 22 model formulations (hypothesis to be tested), starting with predictor variables that are more readly available and ending with the most complex/expensive model. These are:

A. y ~ 0 + x
B. y ~ 1 + x
C. y ~ 1 + x + specOrg + specMinAbund + specOH + clay + texture + landuse + geology + coord


```{r}
# Soil variables
soc_methods <- c("DC", "WCt", "WCc")
soc_methods <- expand.grid(soc_methods, soc_methods, stringsAsFactors = FALSE)[, 2:1]
soc_methods <- soc_methods[!soc_methods[, 1] == soc_methods[, 2], ]

# Predictor variables

#Spectral index highcorrelation
#paste(names(specMinAbund), collapse = " + ")
specMinAbund <- "rKao + rGoe + rHem"

#paste(names(specOrg), collapse = " + ")
specOrg <- "Organics_1 + Organics_2 + Organics_3 + Organics_4 + Organics_5 + Organics_6 + Organics_7 + Organics_8 + Organics_9 + Organics_10 + Organics_11 + Organics_12 + Organics_13 + Organics_14 + Organics_15 + Organics_16 + Organics_17 + Organics_18 + Organics_19 + Organics_20 + Organics_21 + Organics_22 + Organics_23 + Organics_24"

#paste(names(specOH), collapse = " + ")
specOH <- "Hydroxyl_30 + Hydroxyl_31 + Hydroxyl_32"

landuse <- "FS + U + AA + HE"
texture <- "ME + FI + VF"
geology <- "AV + BV + SE"
coordinates <-  "coord_x + coord_y"

#####

formulas <- lapply(1:nrow(soc_methods), function (i) {
  y <- soc_methods[i, 1]
  x <- soc_methods[i, 2]
  list(
    glue("{y} ~ 0 + {x}"), # A
    glue("{y} ~ 1 + {x}"), # B
    glue("{y} ~ 1 + {x} + I({x}^2)"), # C
    glue("{y} ~ 1 + {x} + I({x}^2) + {specOrg} + {specOH}"), #D
    glue("{y} ~ 1 + {x} + I({x}^2) + {specOrg} + {specOH} + {specMinAbund}"), #E
    glue("{y} ~ 1 + {x} + I({x}^2) + {specOrg} + {specOH} + {x}*clay"), #F
    glue("{y} ~ 1 + {x} + I({x}^2) + {specOrg} + {specOH} + {x}*clay + {specMinAbund}"), #G
    
    glue("{y} ~ 1 + {x} + I({x}^2) + {specOrg} + {specMinAbund} + {specOH} + {x}*clay"), #H
    glue("{y} ~ 1 + {x} + I({x}^2) + {specOrg} + {specMinAbund} + {specOH} + {x}*clay + I({x}^2)*clay + {texture} + {landuse} + {coordinates}"), #I
    glue("{y} ~ 1 + {x} + I({x}^2) + {x}*clay + I({x}^2)*clay + {texture} + {landuse} + {coordinates}"), #J
    
    glue("{y} ~ 1 + {x} + I({x}^2) + {specOrg} + {specMinAbund} + {specOH} + {landuse} + {coordinates}"), #K
    glue("{y} ~ 1 + {x} + I({x}^2) + {x}*clay + I({x}^2)*clay + {texture} + {landuse} + {coordinates}") #L
)
})
names(formulas) <- sapply(formulas, function (x) x[[1]])
```

# TESTE 
```{r}
formulas <- lapply(1:nrow(soc_methods), function (i) {
  y <- soc_methods[i, 1]
  x <- soc_methods[i, 2]
  list(
    glue("{y} ~ 0 + {x}"), # A
    glue("{y} ~ 1 + {x}"), # B
    glue("{y} ~ 1 + {x} + I({x}^2)"), # C

    glue("{y} ~ 1 + {x} + I({x}^2) + {specOrg} + {specOH} + {specMinAbund}"), #D
    
    glue("{y} ~ 1 + {x} + I({x}^2) + {specOrg} + {specMinAbund} + {specOH} + {x}*clay"), #E
    
    glue("{y} ~ 1 + {x} + I({x}^2) + {specOrg} + {specMinAbund} + {specOH} + {x}*clay + I({x}^2)*clay + {texture} + {landuse} + {coordinates}") #F
)
})
names(formulas) <- sapply(formulas, function (x) x[[1]])
```


## Model estimation

We use weighted least squares (WLS) to estimate model parameters. Below, we test the implementation of WLS to
estimate the betas, make predictions and estimate prediction error variances. Estimates are compared with the
output of `lm` and `predict.lm`. This toy exercise is usefull to understand the estimation equations and their
differences compared to ordinary least squares.

```{r, echo = FALSE, eval = FALSE}
# Test observations
vam = caret::createDataPartition(data$DC, p = 0.80, list = FALSE)
test <- data[-vam, ]
tmp_data <- data[vam, ]

# Estimate model using lm
fit <- lm(DC ~ WCc + clay, data = tmp_data, weights = 1 / WCc)
pred <- predict.lm(
  fit, tmp_data[i, c("WCc", "clay")], se.fit = TRUE, weights = ~ 1 / WCc, interval = "prediction", level = 0.95)

# Dependent variable
Y <- tmp_data["DC"] %>% as.matrix()

# Predictor variables
X <- data.frame(x0 = 1, tmp_data[c("WCc", "clay")]) %>% as.matrix()

# Weights
W <- matrix(0, ncol = nrow(X), nrow = nrow(X))
diag(W) <- 1 / X[, "WCc"]

# Estimate betas: (X'WX)⁻¹X'WY
b_hat <- solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% Y %>% drop()
data.frame(b_hat = b_hat, lm = fit$coefficients)

# Residual sum of squares
rss <- sum(diag(W) * (Y - X %*% drop(b_hat))^2) / (nrow(X) - ncol(X))
data.frame(rss, lm = summary(fit)$sigma^2)

# Prediction at a point
x0 <- data.frame(x0 = 1, tmp_data[i, c("WCc", "clay")]) %>% as.matrix()
y_hat <- x0 %*% drop(b_hat)
data.frame(y_hat = y_hat, lm = pred$fit[, "fit"])

# Prediction error variance:
# OLS: RSS * (1 + x0 (X'WX)⁻¹x0')
# WLS: RSS * (c + x0 (X'WX)⁻¹x0')
# c = 1 / w_x0
pev <- rss * diag(x0[, "WCc"] + x0 %*% solve(t(X) %*% W %*% X) %*% t(x0))
data.frame(pev = pev, lm = pred$se.fit^2 + pred$residual.scale^2 * x0[, "WCc"])

# Prediction intervals
bounds <- qt(p = (1 - 0.95) / 2, df = fit$df.residual) * sqrt(pev) * -1
bounds <- y_hat + data_frame(lwr = -bounds, upr = bounds)
data.frame(bounds, lm = pred$fit[, -1])
```

## Model validation

Run leave-one-out cross-validation and compute performance measures:

* MedE: median error
* MedSE: median squared error
* MedAE: median absolute error
* MedSDR: median squared deviation ratio
* AVE: amount of variation explained (or model efficiency)
* mAVE: modified amount of variation explained (or modified model efficiency)

The median is used instead of the mean because it is less sensitive to outliers.

# Results

## Carbon and organic matter data

### Figure 3

Scatter plot matrix of the soil carbon and organic matter content measured using four different analytical 
methods and their relation to the total clay content and class (0-250, 251-500, 501-1000 g kg-1). The solid 
line represents a perfect 1:1 linear relation, while the dashed line is the observed empirical linear relation
between variables.

```{r, eval = FALSE}
# outliers <- c(58, 74, 66, 69, 70, 101, 102, 103, 105)
png("res/fig/scatter-plot-matrix.png", width = 480 * 4, height = 480 * 4, res = 72 * 3)
p1 <- 
  data %>% 
  dplyr::select(DC, WCc, WCt) %>% 
  # dplyr::select(toc, oc, om, tom, clay) %>%
  # mutate(toc = toc / max(toc), oc = oc / max(oc), om = om / max(om), tom = tom / max(tom)) %>%
  lattice::splom(
    groups = data$textura, grid = TRUE, auto.key = list(columns = 3), xlab = '', 
    pscales = 0,
    prepanel.limits = function (x) c(0, 1), abline = c(0, 1),
    panel = function (x, y, ...) {
      m <- min(x, y, na.rm = TRUE)
      s <- diff(range(x, y, finite = TRUE))
      x1 <- (x - m) / s
      y1 <- (y - m) / s
      lattice::panel.splom(x1, y1, ...)
      lattice::panel.lmline(x = x1, y = y1, lty = 'dashed')
    },
    # prepanel.limits = function (x) c(0, 1),
    # prepanel.limits = function (x) c(0, 450),
    varnames = c(expression(atop('DC', '4\u2013162 g kg'^'-1')),
                 expression(atop('WCC', '4\u2013175 g kg'^'-1')),
                 expression(atop('WCT', '8\u2013419 g kg'^'-1')))
    ) #+
                 # expression(atop('OM', 'g dm'^'-3')), expression(atop('TOM', 'g kg'^'-1')),
                 # expression(atop('Clay', 'g kg'^'-1')))) +
  # latticeExtra::layer(panel.abline(a = 0, b = 1)) +
  # latticeExtra::layer(panel.lmline(x = x, y = y, lty = 'dashed'))
# Potential outliers
# p1 <- p1 + latticeExtra::layer(panel.text(x = x[outliers], y = y[outliers], outliers, pos = 1, cex = 0.7))
p1
dev.off()

```

Compute the linear correlation coeeficient between variables. This is used to discuss Figure 4.

```{r}
cor(data[, c("DC", "WCc", "WCt", "clay")]) %>% round(3)
```

```{r}
cor.test(data$DC, data$clay);cor.test(data$WCc, data$clay)
```

Compute the correlation between clay and DC when DC is very low (DC < 50).

```{r}
cor.test(data[data$DC < 2, ]$DC, data[data$C < 2, ]$clay)
cor.test(data[data$WCc < 2, ]$DC, data[data$WCc < 2, ]$clay)
```

## Prediction performance

Identify minimum and maximum values to define a criterion to scale performance measures (MedE, MedAE, and 
MedSE) so that they are comparable.

```{r}
data %>% dplyr::select(DC, WCc, WCt) %>% sapply(min)
data %>% dplyr::select(DC, WCc, WCt) %>% sapply(max)
```

```{r irwls, echo=FALSE, eval=FALSE}
irwls <- 
  function (formula, data, type = c("sd", "var"), maxit = 25, epsilon = 1e-8, trace = TRUE) {
    
    # Unweighted least squares fit
    ols_fit <- lm(formula = formula, data = data)
    deviance0 <- deviance(ols_fit)
    if (trace) {
      print(glue::glue("iteration: 0; deviance: {deviance0}\n"))
    }
    
    # Choose between variance and standard deviation functions
    type <- match.arg(type)
    data$variance <- switch (
      type,
      sd = {
        abs(residuals(ols_fit))
      },
      var = {
        residuals(ols_fit)^2
      })
    
    # Variance or standard deviation function
    
    f <- update.formula(formula, variance ~ .)
    
    # Iterations
    for (i in 1:maxit) {
      
      # Estimate the variance or standard deviation function
      var_fit <- lm(formula = f, data = data)
      data$wgt <- switch (
        type,
        sd = {
          1 / fitted(var_fit)^2
        },
        var = {
          1 / abs(fitted(var_fit))
        })
      
      # Estimate regression coefficients using WLS
      wls_fit <- lm(formula = formula, data = data, weights = wgt)
      
      # Compute residual sum of squares (deviance) of fitted model
      deviance1 <- deviance(wls_fit)
      if (trace) {
        print(glue::glue("iteration: {i}; deviance: {deviance1}\n"))
      }
      
      # Test convergence
      converged <- (abs(deviance1 - deviance0) / (abs(deviance1) + 0.1)) < epsilon
      if (converged) {
        break
      } else {
        deviance0 <- deviance(wls_fit)
        
        data$variance <- switch (
          type,
          sd = {
            abs(residuals(wls_fit))
          },
          var = {
            residuals(wls_fit)^2
          })
      }
    }
    
    # Output
    return (var_fit)
  }
```

```{r}
estimator <- "wls"
```

```{r loocv}
model_fit <- list()
cross_validation <- list()
# i <- 1
for (i in 1:length(formulas)) {
  
  forms <- formulas[[i]]
  y <- data[[soc_methods[i, 1]]]
  resid <- mean(y) - y
  x <- data[[soc_methods[i, 2]]]
  if (estimator == "wls") {
    wgt <- 1 / x # weigths are inversely proportional to the predictor variable
    # wgt <- 1 / x^2 # poorer result
    # wgt <- 1 / sqrt(x) # poorer result
  } else if (estimator == "ols") {
    wgt <- rep(1, length(x))
  }
  
  fit <- list()
  loocv <- list()
  # j <- 7
  for (j in 1:length(forms)) {
    f <- forms[[j]]
    
    # Record fitted models for latter reuse
    fit[[j]] <- lm(formula = f, data = data, weights = wgt)
    
    out <- data.frame(pred = NA_real_, pev = NA_real_)
    # Leave-one-out cross-validation
    # k <- 1
    print(f)
    for (k in 1:nrow(data)) {
      # Weighted least squares regression
      # Predict value of dependent variable at new observation and return components of the error variance
      # The residual standard deviation needs to be 'scaled' using the weight of the new observation
      lm_fit <- lm(formula = f, data = data[-k, ], weights = wgt[-k])
      pred <- predict.lm(object = lm_fit, newdata = data[k, ], se.fit = TRUE)
      out[k, ] <- c(pred$fit, pred$se.fit^2 + pred$residual.scale^2 / wgt[k])
      # IRWLS
      # irls_fit <- irwls(formula = f, data = data[-k, ], type = "sd", maxit = 100, epsilon = 1e-3)
      # lm_fit <- lm(formula = f, data = data[-k, ], weights = 1 / fitted(irls_fit)^2)
      # pred <- predict.lm(object = lm_fit, newdata = data[k, ], se.fit = TRUE)
      # wgt <- predict.lm(object = irls_fit, newdata = data[k, ])^2
      # out[k, ] <- c(pred$fit, pred$se.fit^2 + pred$residual.scale^2 * wgt)
    }
    
    # Cross-validation statistics
    # Measures (MedE, MedSE, and MedAE) are scaled using the maximum observed value of the dependent variable
    error <- out$pred - y
    error_sqr <- error * error
    error_abs <- abs(error)
    denom <- max(y) # scale measures
    loocv[[j]] <- data.frame(
      
      # Model
      f = f,
      p = LETTERS[j] %>% as.factor(),
      y = soc_methods[i, 1],
      x = soc_methods[i, 2],
      m = glue("{soc_methods[i, 1]} ~ {soc_methods[i, 2]}") %>% toupper() %>% as.factor(),
      
      # Median error measures
      MedE = median(error) / denom,
      MedAE = median(error_abs) / denom,
      MedSE = median(error_sqr) / (denom * denom),
      # MedSDR = 1 - qchisq(p = 0.5, df = 1) + median(error_sqr / out$pev), # DEPRECATED
      MedSDR = median(error_sqr / out$pev),
      # MedSDR = mean(error_sqr / out$pev),
      
      # Model efficiency
      AVE = 1 - (sum(error_sqr) / sum(resid * resid)),
      mAVE = 1 - (sum(error_abs) / sum(abs(resid))) # less sensitive to outliers than AVE
    )
  }
  names(fit) <- forms
  model_fit[[i]] <- fit
  cross_validation[[i]] <- do.call(rbind, loocv)
}
cross_validation <- do.call(rbind, cross_validation)
write.csv(cross_validation, "result/loocv.csv")
```

Prediction bias of the most complex models.

```{r}
cross_validation %>% filter(p == "G") %>% dplyr::select(MedE) %>% c()
```

Spread of prediction errors for DC ~ WCc and DC ~ WCt accross model formulations.

```{r}
cross_validation %>% filter(m == "DC ~ WCC" | m == "DC ~ WCT") %>% dplyr::select(MedSE, MedAE) %>% 
  summarise(mean(MedSE), mean(MedAE)) %>% round(3)
```

Spread of prediction errors for DC ~ WCC and DC ~ WCC accross model formulations.

```{r}
cross_validation %>% filter(m == "DC ~ WCC" | m == "DC ~ WCT") %>% filter(p == "A" | p == "G") %>% 
  dplyr::select(m, p, MedSE, MedAE)
```

Median squared deviation ratio of the least and most complex models.

```{r, fig.asp = 1}
cross_validation %>% 
  filter(p == "A" | p == "S") %>%
  dplyr::select(m, MedSDR) %>% 
  group_by(m) %>% 
  summarise(dif = diff(MedSDR)) %>% 
  arrange(dif)
```

```{r, fig.asp = 1}
cross_validation %>% 
  filter(p == "A" | p == "S") %>%
  dplyr::select(m, p, MedSDR)
```

### Figure 5

Leave-one-out cross-validation performance of PTFs estimated via WLS. Line color and type indicate the 
dependent and predictor variable, respectively. Measurement units of MedE, MedAE, and MedSE are % -- for 
TOC, OC, and TOM -- and g dm-3 -- for OM, while MedSDR, AVE, and mAVE are unitless performance measures. Model
formulations are described in Table 2. Performance measures are defined in Table 3.

```{r loocvplot, eval = FALSE}
col <- c("dodgerblue", "magenta", "olivedrab") %>% rep(each = 2)
lty <- c(1, 2, 3, 4, 2, 3)
p1 <-
  xyplot(
    AVE + mAVE + MedAE + MedSE + MedE + MedSDR ~ p, data = cross_validation, groups = m,
    type = "l", col = col, lty = lty, lwd = 3, xlab = "Model formulation", ylab = "",
    key = list(lines = list(col = col, lty = lty, lwd = 3), columns = 4, 
               text = list(levels(cross_validation$m))),
    scales = list(x = list(relation = "same"), y = list(relation = "free")), layout = c(2, 3),
    panel = function (...) {
      panel.grid(h = -1, v = -1)
      panel.xyplot(...)
    })
png(glue("result/cross-validation-{estimator}.png"), width = 480 * 4, height = 480 * 5, res = 72 * 3)
p1
dev.off()
```

Save fitted models and print model summary.

```{r}
save(model_fit, file = "../result/model_fit.rda")
lapply(model_fit, function (x) {
  lapply(x, summary)
})
```


